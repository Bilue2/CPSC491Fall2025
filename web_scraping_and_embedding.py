# -*- coding: utf-8 -*-
"""Web scraping and Embedding

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/170KnAshvGxRTfPCpnihf7_Yc8CYfO2BO
"""

!pip install PyMuPDF
!pip install jsonlines
from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

import fitz
import jsonlines

def pdf_to_jsonl(pdf_path, jsonl_path):
  with fitz.open(pdf_path) as doc:
    with jsonlines.open(jsonl_path, mode='w') as writer:
      for page in doc:
        text = page.get_text()
        writer.write({"text": text})

pdf_path = 'foia-report-2023.pdf'
jsonl_path = 'output.jsonl'
pdf_to_jsonl(pdf_path, jsonl_path)
pdf_path
jsonl_path

import requests
from bs4 import BeautifulSoup

# Define the URL of the page you want to scrape
url = 'https://fedscoop.com/fcc-proposes-72-hour-breach-reporting-timeline-for-emergency-alert-system-participants/'
html=requests.get(url)
soup = BeautifulSoup(html.text, parser="html.parser")
course_cards=soup.find_all("div", class_="ad__inner")
for course in course_cards:
  print(course)

!pip install openai
!pip install openai requests beautifulsoup4
import openai
import os
import requests
from bs4 import BeautifulSoup

# Set up OpenAI API key
openai.api_key = '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'  # Replace with your actual API key
# Step 1: Scrape the webpage
url = 'https://fedscoop.com/fcc-proposes-72-hour-breach-reporting-timeline-for-emergency-alert-system-participants/'
html = requests.get(url)
soup = BeautifulSoup(html.text, "html.parser")

# Extract content from the web page (you may need to adjust the tag/class to get the full article text)
content = soup.find_all("div", class_="ad__inner")
text_data = [element.get_text(strip=True) for element in content]

# Combine text data into a single string for embedding
text_content = " ".join(text_data)

# Check if text content was found
if not text_content:
    print("No content found in the specified HTML tags.")
else:
    # Step 2: Generate Embeddings using OpenAI's API
    try:
        response = openai.Embedding.create(
            model="text-embedding-ada-002",
            input=text_content
        )

        # Extract the embedding vector from the response
        embedding = response['data'][0]['embedding']

        print("Text content:\n", text_content)
        print("\nGenerated Embedding:\n", embedding)

    except Exception as e:
        print("An error occurred while generating embeddings:", e)

from sentence_transformers import SentenceTransformer
from google.colab import userdata
userdata.get("Token")
from google.colab import userdata
userdata.get("openai")
# Load a pre-trained model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Your text data to be embedded
text_data = ["This is an example text for embedding generation."]

# Generate embeddings
embeddings = model.encode(text_data)

# Print the generated embedding
for text, emb in zip(text_data, embeddings):
    print(f"Text: {text}\nEmbedding: {emb[:5]}...")  # Showing the first 5 values for brevity

import requests
from bs4 import BeautifulSoup

# Define the URL
url = 'https://fedscoop.com/fcc-proposes-72-hour-breach-reporting-timeline-for-emergency-alert-system-participants/'

# Send a GET request to the URL
response = requests.get(url)

# Check if the request was successful
if response.status_code == 200:
    # Parse the HTML content using BeautifulSoup
    soup = BeautifulSoup(response.text, 'html.parser')

    # Find the main content (adjust based on the webpage structure)
    # Here we are looking for divs or paragraphs that might contain the main article text
    content_divs = soup.find_all("div", class_="single-article__container js-single-article-content")

    # Extract text and combine into a single string
    content_text = " ".join([div.get_text(strip=True) for div in content_divs])

    # Display a sample of the scraped text to verify
    print("Scraped Content:\n", content_text[:500], "...\n")  # Display first 500 characters
else:
    print("Failed to retrieve the webpage. Status code:", response.status_code)

from sentence_transformers import SentenceTransformer

# Initialize the embedding model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Generate embeddings if content was found
if content_text:
    # Sentence Transformers can handle long strings, but you might want to split by sentences or paragraphs if needed.
    embeddings = model.encode([content_text])

    # Display a portion of the embedding
    print("Generated Embedding (first 10 values):\n", embeddings[0][:10])  # Showing first 10 values for brevity
else:
    print("No content was scraped to generate embeddings.")

import pandas as pd

# Create a DataFrame to store the text and its corresponding embedding
data = {
    "content": [content_text],
    "embedding": [embeddings[0].tolist()]  # Convert numpy array to list for easier storage
}

df = pd.DataFrame(data)

# Save to CSV
df.to_csv('webpage_content_embeddings.csv', index=False)
print("Embeddings saved to 'webpage_content_embeddings.csv'.")

!pip install pinecone-client
import pandas as pd
import pinecone
import os
from pinecone import Pinecone, ServerlessSpec
pc = Pinecone(
        api_key=os.environ.get("pcsk_2nph8k_GxBdMtc1R3gGHBLHvp9dGMejApP5ADPvYfQHMo5jLFPY7pGDerUw5ZUqnH3JXBk")
    )
# Define index name and vector dimensionality
index_name = "webpage-embeddings"
dimension = 384  # Adjust if using a different model

# Check if the index already exists; if not, create it
if index_name not in pinecone.list_indexes():
    pinecone.create_index(name=index_name, dimension=dimension)

# Connect to the index
index = pinecone.Index(index_name)

import os
from google.colab import files

# Step 1: Upload Files
uploaded = files.upload()  # Select multiple files to upload

# Step 2: Create a Directory (if it doesn't exist)
directory_path = "/content/fcc"  # Replace "all_pdfs" with your desired directory name
if not os.path.exists(directory_path):
    os.makedirs(directory_path)

# Step 3: Move Files to the Directory
for file_name in uploaded.keys():
    # Move each uploaded file to the directory
    source_path = f"/content/{file_name}"
    destination_path = os.path.join(directory_path, file_name)
    os.rename(source_path, destination_path)

print(f"All files have been moved to: {directory_path}")



import os
import fitz  # PyMuPDF
from google.colab import files
from sentence_transformers import SentenceTransformer

# Step 1: Upload Multiple PDF Files
uploaded = files.upload()  # Upload multiple files

# Step 2: Create Directory for Storing PDFs (if it doesn't exist)
directory_path = "/content/fcc"  # Desired directory path for PDFs
if not os.path.exists(directory_path):
    os.makedirs(directory_path)

# Step 3: Move the Uploaded Files to the Directory
for file_name in uploaded.keys():
    source_path = f"/content/{file_name}"
    destination_path = os.path.join(directory_path, file_name)
    os.rename(source_path, destination_path)

print(f"All files have been moved to: {directory_path}")

# Step 4: Extract Text from All PDFs in the Directory
pdf_text = ""  # Initialize an empty string to store all extracted text
for file_name in os.listdir(directory_path):
    if file_name.endswith('.pdf'):
        pdf_path = os.path.join(directory_path, file_name)
        doc = fitz.open(pdf_path)  # Open the PDF using PyMuPDF
        for page_num in range(doc.page_count):  # Loop through all pages
            page = doc.load_page(page_num)  # Load each page
            pdf_text += page.get_text()  # Extract text from the page
        print(f"Text extracted from {file_name}")

# Step 5: Create Embeddings from the Extracted Text
model = SentenceTransformer('all-MiniLM-L6-v2')  # Load a pre-trained embedding model
embeddings = model.encode([pdf_text])  # Generate embeddings for the combined text

print("Embeddings created for all PDFs.")
print(f"Shape of embeddings: {embeddings.shape}")

# Check the shape of the embeddings
print(f"Shape of embeddings: {embeddings.shape}")  # Should return something like (1, embedding_dimension)

# Optionally, print a snippet of the embeddings to see the numerical values
print("Sample Embedding Values:")
print(embeddings[0][:10])  # Show first 10 values of the embedding for verification

# Example: Embedding two different PDFs
pdf_text_1 = "Text from the first PDF..."
pdf_text_2 = "Text from the second PDF..."

# Create embeddings for both texts
embedding_1 = model.encode([pdf_text_1])
embedding_2 = model.encode([pdf_text_2])

# Compare the similarity between the two embeddings (cosine similarity)
from sklearn.metrics.pairwise import cosine_similarity

similarity = cosine_similarity([embedding_1[0]], [embedding_2[0]])  # Compare first embeddings
print(f"Cosine similarity between the two embeddings: {similarity[0][0]}")

# Query to search within the PDF text
query = "search term or question"
query_embedding = model.encode([query])

# Search the similarity of the query against the PDF embeddings
query_similarity = cosine_similarity(query_embedding, embeddings)

# Print the most similar PDF based on query
print(f"Most relevant document index based on query: {query_similarity.argmax()}")

# Print the extracted text from all PDFs
print(pdf_text)

# Save the extracted text to a text file
text_file_path = "/content/fcc/extracted_text.txt"
with open(text_file_path, 'w') as text_file:
    text_file.write(pdf_text)

print(f"Extracted text has been saved to {text_file_path}")

# Generate embeddings for the combined extracted text
embeddings = model.encode([pdf_text])
embeddings

import csv
import numpy as np

# Example text and embeddings data
texts = ["First document text", "Second document text", "Third document text"]
embeddings = [
    np.random.rand(384),  # Random 384-dimensional embedding for the first text
    np.random.rand(384),  # Random 384-dimensional embedding for the second text
    np.random.rand(384)   # Random 384-dimensional embedding for the third text
]

# Path to save the CSV file
csv_file_path = "/content/fcc/embeddings.csv"

# Write the data to a CSV file
with open(csv_file_path, mode='w', newline='') as csv_file:
    csv_writer = csv.writer(csv_file)

    # Write header
    csv_writer.writerow(["text", "embedding"])

    # Write each text and embedding as a row
    for text, embedding in zip(texts, embeddings):
        # Flatten embedding array into a comma-separated string for CSV
        embedding_str = ','.join(map(str, embedding))

        # Write text and embedding to CSV
        csv_writer.writerow([text, embedding_str])

print(f"Data has been written to {csv_file_path}")

import json
import numpy as np

# Example text and embeddings data
texts = ["First document text", "Second document text", "Third document text"]
embeddings = [
    np.random.rand(384).tolist(),  # Convert numpy array to list for JSON serialization
    np.random.rand(384).tolist(),
    np.random.rand(384).tolist()
]

# Path to save the JSON file
json_file_path = "/content/fcc/embeddings.json"

# Create a list of dictionaries to store the data
data = [{"text": text, "embedding": embedding} for text, embedding in zip(texts, embeddings)]

# Write the data to a JSON file
with open(json_file_path, mode='w') as json_file:
    json.dump(data, json_file, indent=4)  # Pretty print with indent=4

print(f"Data has been written to {json_file_path}")

import os
!pip install PyPDF2
from PyPDF2 import PdfReader

# Step 1: Directory Setup
# Directory where the FCC PDFs are stored
fcc_directory = "/content/fcc"

# Ensure the directory exists
if not os.path.exists(fcc_directory):
    raise FileNotFoundError(f"The directory '{fcc_directory}' does not exist.")

# Step 2: Function to Scrape Text from All PDFs
def scrape_fcc_pdfs(directory_path):
    """
    Scrapes text from all PDF files in the specified directory.

    Args:
        directory_path (str): Path to the directory containing PDFs.

    Returns:
        dict: A dictionary where keys are filenames and values are extracted text.
    """
    pdf_texts = {}

    # Iterate through all files in the directory
    for file_name in os.listdir(directory_path):
        if file_name.endswith(".pdf"):  # Ensure file is a PDF
            pdf_path = os.path.join(directory_path, file_name)
            try:
                # Extract text from the PDF
                reader = PdfReader(pdf_path)
                text = ""
                for page in reader.pages:
                    text += page.extract_text()

                # Save extracted text into the dictionary
                pdf_texts[file_name] = text
                print(f"Scraped text from: {file_name}")
            except Exception as e:
                print(f"Failed to process {file_name}: {e}")

    return pdf_texts

# Step 3: Run the Scraper and Store Results
fcc_texts = scrape_fcc_pdfs(fcc_directory)

# Step 4: Save Extracted Text to a File
output_file = "/content/fcc_texts.txt"
with open(output_file, "w", encoding="utf-8") as f:
    for file_name, text in fcc_texts.items():
        f.write(f"File: {file_name}\n")
        f.write(f"Content:\n{text}\n")
        f.write("=" * 50 + "\n")  # Separator for each file

print(f"Scraped FCC texts saved to: {output_file}")

import openai

# Replace with your OpenAI API key
openai.api_key = "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"

# Generate embeddings
fcc_embeddings = []
for file_name, text in output_file.items():
    response = openai.Embedding.create(
        model="text-embedding-ada-002",
        input=text
    )
    embedding = response['data'][0]['embedding']
    fcc_embeddings.append({
        "file_name": file_name,
        "embedding": embedding,
        "text": text
    })

print("Generated embeddings:")
for item in fcc_embeddings:
    print(f"File: {item['file_name']}, Embedding Length: {len(item['embedding'])}")

!pip install pinecone
from pinecone import Pinecone, ServerlessSpec

# Initialize Pinecone
pc = Pinecone(api_key="pcsk_7HHHUx_TU58ZsjDgjzjE3g8BWfsMuTJDptkemPRE5mQkuGwWBufEsPLnR1uzj5BBWPTDTm")

# Create or connect to an index
index_name = "fcc-index"
if index_name not in pc.list_indexes().names():
    pc.create_index(
        name=index_name,
        dimension=1536,
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )
index = pc.Index(index_name)

# Insert data
for doc in fcc_embeddings:
    index.upsert(
        vectors=[(doc[""], doc["embedding"], {"text": doc["text"]})],
        namespace="fcc-data"
    )
print("Data stored in Pinecone.")